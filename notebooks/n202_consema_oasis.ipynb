{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consema on OASIS data\n",
    "\n",
    "- Author: Luca Mossina. IRT Saint Exup√©ry, Toulouse, France\n",
    "\n",
    "**Scope**: test the full pipeline for Conformal Prediction on OASIS\n",
    "\n",
    "- data: OASIS\n",
    "- predictor: Universeg\n",
    "- nonconformity score: thresholding, n. of dilations, radius of structuring element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from consema.conformal import Conformalizer\n",
    "\n",
    "from universeg import universeg  # installed via Makefile\n",
    "\n",
    "model = universeg(pretrained=True)\n",
    "\n",
    "GPUNAME = \"cuda:0\"\n",
    "device_str = GPUNAME if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "_ = model.to(device)\n",
    "\n",
    "RANDOM_SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data, predictors and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "from benchmarkerie.datasets import ExtendedOASISDataset\n",
    "from benchmarkerie.datasets import make_universeg_predictions\n",
    "\n",
    "# from typing import Literal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupOASIS(n_support_samples, label, random_seed, device=\"cpu\"):\n",
    "    data_support = ExtendedOASISDataset(\n",
    "        split=\"support\", label=label, random_seed=random_seed\n",
    "    )\n",
    "    data_calib = ExtendedOASISDataset(\n",
    "        split=\"calibration\", label=label, random_seed=random_seed\n",
    "    )\n",
    "    data_test = ExtendedOASISDataset(split=\"test\", label=label, random_seed=random_seed)\n",
    "\n",
    "    print(f\"{len(data_support) = }\")\n",
    "    print(f\"{len(data_calib) = }\")\n",
    "    print(f\"{len(data_test) = }\")\n",
    "\n",
    "    assert n_support_samples <= len(data_support), \"Not enough support samples\"\n",
    "\n",
    "    support_images, support_labels = zip(\n",
    "        *itertools.islice(data_support, n_support_samples)\n",
    "    )\n",
    "    support_images = torch.stack(support_images).to(device)\n",
    "    support_labels = torch.stack(support_labels).to(device)\n",
    "\n",
    "    return data_support, data_calib, data_test, support_images, support_labels\n",
    "\n",
    "\n",
    "data_support, data_calib, data_test, support_images, support_labels = setupOASIS(\n",
    "    24,\n",
    "    0,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    device=device_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_support_samples = 48  # high performance predictor\n",
    "# n_support_samples = 12  # lower performance predictor\n",
    "n_support_samples = 24  # middle performance predictor\n",
    "\n",
    "# LABEL = 0 # background\n",
    "LABEL = 1  # left-hand side of brain image (todo: look up the actual label)\n",
    "# LABEL = 10\n",
    "\n",
    "data_support, data_calib, data_test, support_images, support_labels = setupOASIS(\n",
    "    n_support_samples,\n",
    "    LABEL,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    device=device_str,\n",
    ")\n",
    "\n",
    "\n",
    "LOAD_PREDS = False\n",
    "SAVE_PREDS = False\n",
    "\n",
    "if SAVE_PREDS:\n",
    "    load_dotenv()\n",
    "    PREDS_DIR = os.getenv(\"EXPERIMENTS_DIR\", \"experiments\")\n",
    "else:\n",
    "    PREDS_DIR = None\n",
    "\n",
    "\n",
    "if LOAD_PREDS:\n",
    "    load_dotenv()\n",
    "    PREDS_DIR = os.getenv(\"EXPERIMENTS_DIR\", \"experiments\")\n",
    "    # Load predictions for calibration and test datasets\n",
    "    calib_preds = np.load(f\"{PREDS_DIR}/calib_nsup_{n_support_samples}.npz\")\n",
    "    # test_preds = np.load(f\"{PREDS_DIR}/test.npz\")\n",
    "\n",
    "    calib_images = calib_preds[\"images\"]\n",
    "    calib_gt_masks = calib_preds[\"gt_masks\"]\n",
    "    calib_predictions = calib_preds[\"preds\"]\n",
    "\n",
    "    # test_images = test_preds['images']\n",
    "    # test_gt_masks = test_preds['gt_masks']\n",
    "    # test_predictions = test_preds['preds']\n",
    "\n",
    "    print(\"Calibration predictions loaded.\")\n",
    "    print(\"Test predictions loaded.\")\n",
    "\n",
    "# # Save predictions for calibration and test datasets in single files\n",
    "calib_images, calib_gt_masks, calib_predictions = make_universeg_predictions(\n",
    "    dataset=data_calib,\n",
    "    dataset_name=f\"calib_nsup_{n_support_samples}\",\n",
    "    device_str=device_str,\n",
    "    support_images=support_images,\n",
    "    support_labels=support_labels,\n",
    "    universeg_model=model,\n",
    "    save_to=PREDS_DIR,\n",
    ")\n",
    "\n",
    "\n",
    "# test_images, test_gt_masks, test_predictions = make_universeg_predictions(\n",
    "#     dataset=data_test,\n",
    "#     dataset_name=f\"test_nsup_{n_support_samples}\",\n",
    "#     device_str=device_str,\n",
    "#     support_images=support_images,\n",
    "#     support_labels=support_labels,\n",
    "#     universeg_model=model,\n",
    "#     save_to=PREDS_DIR,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "DO_LOAD = False\n",
    "\n",
    "if DO_LOAD:\n",
    "    # Load predictions for calibration and test datasets\n",
    "    calib_preds = np.load(f\"{PREDS_DIR}/calib_nsup_{n_support_samples}.npz\")\n",
    "\n",
    "    calib_images = calib_preds[\"images\"]\n",
    "    calib_gt_masks = calib_preds[\"gt_masks\"]\n",
    "    calib_predictions = calib_preds[\"preds\"]\n",
    "\n",
    "    # test_images = test_preds['images']\n",
    "    # test_gt_masks = test_preds['gt_masks']\n",
    "    # test_predictions = test_preds['preds']\n",
    "\n",
    "    print(\"Calibration predictions loaded.\")\n",
    "    print(\"Test predictions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize False Negatives: we want to control their quantity\n",
    "\n",
    "Via conformal prediction, we control how many false negative we will have, on average, in our test inferences (e.g. when deployed in production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.plots import visualize_false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, (image, prediction, gt) in enumerate(\n",
    "    zip(calib_images, calib_predictions, calib_gt_masks)\n",
    "):\n",
    "    if it >= 2:\n",
    "        break\n",
    "    visualize_false_negatives(image[0], gt[0], prediction[0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the **red points** are false negatives, that is, points that belong to the ground-truth masks but were not predicted by the algorithm.\n",
    "\n",
    "Using conformal prediction, we want to \"extend\" the prediction area (in green) with a conformal margin so that we limit our prediction errors, that is, we reduce the number of false negative with high probability at a significance level chosen by the user.\n",
    "\n",
    "The price to pay are more false positives: in this case, we can imagine that false positives push the users to be more conservative and they are \"safe\" errors.\n",
    "\n",
    "If there are too many false positives, then the prediction becomes operationally useless; this tradeoff is controlled by the user and their predictive model: worse models will have more false negatives, hence larger conformal margins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.morphology import (\n",
    "    dilation_metrics,\n",
    ")\n",
    "from consema.conformal import thresholding_score\n",
    "from consema.plots import plot_margin_and_recovered, visualize_false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = 10\n",
    "bintruth = calib_gt_masks[idx_] > 0.5\n",
    "binpred = calib_predictions[idx_] > 0.5\n",
    "softpred = calib_predictions[idx_]\n",
    "\n",
    "cov_threshold = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(softpred[0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding nonconformity score\n",
    "\n",
    "Sources:\n",
    "- LAC (Least Ambiguous Set-Valued Classifiers), [Sadinle et al. (2019)](https://arxiv.org/abs/1609.00451)\n",
    "- CRC (Conformal Risk Control), [Angelopoulos et al. (2022)](https://arxiv.org/abs/2208.02814)\n",
    "\n",
    "**Thresholding the softmax** or sigmoid scores for **classification** follows from the theory developed in Sadinle et al. (2019), where they work with generic probability distribution, not specifically with neural networks. In this case, the probability estimates (e.g. softmax scores) are known to be not calibrated, and some properties (e.g. optimal size of prediction sets) do not necessarily hold. \n",
    "\n",
    "As for **binary image segmentation** with an underlying pixel-wise classifier, this was introduced as an application of their CRC algorithm by Angelopoulos et al. (2022): they do not explicitly mention nonconformity scores, although it follows immediately from their definition of risk and computation of $\\hat{\\lambda}$, for the case of binary (conformal) losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_threshold, dilated_threshold_mask = thresholding_score(\n",
    "    gt_mask_=bintruth,\n",
    "    soft_pred_mask_=softpred,\n",
    "    # se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "\n",
    "print(f\"{nc_threshold = :.9f}\")\n",
    "metrics = dilation_metrics(dilated_threshold_mask[0], binpred[0])\n",
    "print(f\" --- THRESH: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "\n",
    "coverage_tensor = np.multiply(dilated_threshold_mask, bintruth)\n",
    "coverage = np.sum(coverage_tensor) / np.count_nonzero(bintruth)\n",
    "print(f\"{coverage = :.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_threshold_mask[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_margin_and_recovered(\n",
    "    binpred[0],\n",
    "    bintruth[0],\n",
    "    dilated_threshold_mask[0],\n",
    "    plot_hard_margin=False,\n",
    "    figsize=(6, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological prediction sets: dilation\n",
    "\n",
    "This is part of the original contribution of the paper: building statistically valid prediction sets under minimal information.\n",
    "- No softmax/sigmoid scores necessary\n",
    "- usable on blackbox predictor, e.g. hidden behind API, MLaaS, etc.\n",
    "- only need a small dataset of labeled, production-like data to measure uncertainty\n",
    "- literature available: mathematical morphology for computer vision (without deep learning)\n",
    "\n",
    "I introduce two cases:\n",
    "- (3x3) structuring element, either square or cross: iteratively dilate the mask as dilated at previous iterations until $\\tau \\times 100 \\%$ of the ground-truth pixels are recovered. Applying several dilations (or other) is also known as **depth** of the dilation. This will constitue the nonconformity score, the higher the worse the prediction was.\n",
    "- variable-size disk: as above, but only apply one dilation to the predicted mask, with the radius of the disk increasing at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.morphology import (\n",
    "    dilation_score_variable_disk,\n",
    "    dilation_score_fixed_disk,\n",
    ")\n",
    "\n",
    "se_params = dict(strict_radius=True)  # [3 X 3] cross\n",
    "# se_params = dict(strict_radius=False)  # [3 X 3] square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative dilations\n",
    "\n",
    "Nonconcormity score = depth, or number of repeated dilations.\n",
    "- Using fixed-size structuring element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Fixed disk\n",
    "nc_fixed, dilated_mask_fixed = dilation_score_fixed_disk(\n",
    "    gt_mask_=bintruth[0],\n",
    "    pred_mask_=binpred[0],\n",
    "    se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "print(f\"{nc_fixed = }\")\n",
    "metrics = dilation_metrics(dilated_mask_fixed, binpred[0])\n",
    "print(f\" ---  FIXED: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "plot_margin_and_recovered(\n",
    "    binpred[0],\n",
    "    bintruth[0],\n",
    "    dilated_mask_fixed,\n",
    "    input_image=calib_images[idx_][0],\n",
    "    softprediction=softpred[0],\n",
    ")\n",
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_mask_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable-size disk as structuring element\n",
    "\n",
    "Instead of repeating dilations incrementally, on can obtain a similar results modifying the radius of the structuring element used for dilations.\n",
    "\n",
    "Computationally, it should be worse than fixed-size for big radii: a naive implementation requires a number a computations that grows quadratically with the size (H x W) of the structuring element.\n",
    "\n",
    "For binary dilations (our case), one can use a convolution followed by a max: this can be a good option if working directly with torch and tensors; with large images, we could expect comp. gains via gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Variable disk\n",
    "nc_variable, dilated_mask_variable = dilation_score_variable_disk(\n",
    "    gt_mask_=bintruth[0],\n",
    "    pred_mask_=binpred[0],\n",
    "    se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "print(f\"{nc_variable = }\")\n",
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_mask_variable)\n",
    "\n",
    "metrics = dilation_metrics(dilated_mask_variable, binpred[0])\n",
    "print(f\" ---  VARIA: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "plot_margin_and_recovered(binpred[0], bintruth[0], dilated_mask_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal loop: compute scores and quantiles\n",
    "\n",
    "Conformal prediction (CP) is usually presented as a predictive method to build statistically valid prediction sets (or intervals) that will contain, with high probability, the true value of a target $Y$ being predicted by some ML model as $\\hat{Y} = \\hat{f}(X)$; the features X could be tabular data, images, etc.\n",
    "\n",
    "However, we have another useful piece of information: the actual errors (nonconformity scores), their size, their distribution.\n",
    "CP can also be used as a diagnostic tool: the users choose a score (or make their own), a tolerable risk level, and then they could:\n",
    "- compare several architectures, trained on the same data: the one with the smallest scores (and prediction set) could be preferable to another with slightly better performance metrics (precision, mAP, etc.)\n",
    "- interpret the errors: in production, how and where the model errs.\n",
    "- interpret the size and shape of the prediction set: for regression or classification, this is the width and cardinality or pred. sets. For segmentation or object detection, this could be the size of the conformal margins, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Split conformal prediction procedure:\n",
    "\n",
    "\n",
    "1. Setup a pre-trained model, or train your own\n",
    "2. setup a parametrized prediction set (hence a nonconformity score): $C_{\\lambda}(X)$\n",
    "3. Fix an acceptable error probability $\\alpha$, also called \"risk\", \"nominal error\", \"nominal risk\", etc.\n",
    "- can be written as $\\epsilon$\n",
    "- $1-\\alpha$ is known as the (nominal) coverage of our conformalized predictor: with confidence $1-\\alpha$, the sets will contain the true value of $Y$ (this is a frequentist object, so some care is necessary when building and interpreting prediction sets).\n",
    "4. Get some **production-like** data $D_{\\text{cal}}$, often called \"calibration data\": labeled data, not used during training, that represents the data distribution fed into your model when deployed in production\n",
    "5. Compute nonconformity scores $r(X,Y)$ on $D_{\\text{cal}}$\n",
    "6. Compute the **adjusted** empirical quantile of order $(1-\\alpha)(1+\\frac{1}{n})$\n",
    "- sort the scores in ascending order\n",
    "- compute the quantile as $\\hat{\\lambda_{\\alpha}} = \\lceil (1 - \\alpha)(n+1) \\rceil$-th largest score\n",
    "7. At inference, compute $C_{\\hat{\\lambda}}(X)$ by plugging $\\hat{\\lambda_{\\alpha}}$ into $C_{\\lambda}(X)$ \n",
    "\n",
    "Note that $C_{\\lambda}(X)$ can be *any* prediction set, as long as the they are nested:\n",
    "- $C_{\\lambda_0}(X) \\subseteq C_{\\lambda_1}(X)$ for all $\\lambda_0 \\leq \\lambda_1$\n",
    "- as $\\lambda$ grows, so does the prediction set. \n",
    "- In the worst-case scenario, we can retrieve the trivial set $C_{\\lambda_{max}}(X) = \\mathcal{Y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize margin: use a gradient to show successive dilations and recovered pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.plots import margin_gradient_visu\n",
    "\n",
    "margin_var = margin_gradient_visu(\n",
    "    binpred[0], \"variable_disk\", 10, None, False, se_params\n",
    ")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].imshow(margin_var, cmap=\"cividis\", interpolation=\"none\")\n",
    "margin_fixed = margin_gradient_visu(binpred[0], \"fixed_disk\", 10, None, True, se_params)\n",
    "ax[1].imshow(margin_fixed, cmap=\"cividis\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from consema.conformal import recovered_pixels_bin_array\n",
    "\n",
    "_grad = margin_gradient_visu(binpred[0], \"variable_disk\", 10, 20, True, se_params)\n",
    "\n",
    "# Assuming _grad is your H x W array\n",
    "H, W = _grad.shape\n",
    "rgba_image = np.zeros((H, W, 4), dtype=np.uint8)  # Create an RGBA array\n",
    "\n",
    "# Normalize the gradient to the range [0, 1]\n",
    "normalized_grad = _grad / np.max(_grad)\n",
    "\n",
    "# Set all pixels that are non-zero in the recovered mask to red with full opacity\n",
    "recovered = recovered_pixels_bin_array(bintruth[0], binpred[0], dilated_mask_variable)\n",
    "\n",
    "# Define blending factor and colors\n",
    "alpha_blend = 0.7  # Controls red intensity (0 = no red, 1 = fully red)\n",
    "\n",
    "# Assign colors: white for zeros, blue for non-zero entries\n",
    "nonmargin_indices = np.where(_grad > 0)\n",
    "rgba_image[..., 0] = (1 - normalized_grad) * 255  # Red channel\n",
    "rgba_image[..., 1] = (1 - normalized_grad) * 255  # Green channel\n",
    "rgba_image[..., 2] = 255  # Blue channel\n",
    "rgba_image[..., 3] = (normalized_grad * 255).astype(\n",
    "    np.uint8\n",
    ")  # Alpha channel (transparency)\n",
    "\n",
    "# Blend red for recovered pixels\n",
    "recovered_indices = np.where(recovered == 1)\n",
    "rgba_image[recovered_indices] = (\n",
    "    alpha_blend * np.array([255, 0, 0, 255])  # Red with full opacity\n",
    "    + (1 - alpha_blend) * rgba_image[recovered_indices]\n",
    ").astype(np.uint8)\n",
    "\n",
    "# Visualize the RGBA image\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# Plot ground truth mask on the left\n",
    "ax[0].imshow(bintruth[0], cmap=\"gray\", interpolation=\"none\")\n",
    "ax[0].set_title(\"Ground-truth mask\", fontsize=10)\n",
    "# ax[0].axis(\"off\")\n",
    "\n",
    "# Plot softmax prediction\n",
    "ax[1].imshow(softpred[0], cmap=\"viridis\", interpolation=\"none\")\n",
    "ax[1].set_title(\"Softmax Prediction\", fontsize=10)\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "# plot hard predicted mask\n",
    "ax[2].imshow(binpred[0], cmap=\"gray\", interpolation=\"none\")\n",
    "# ax[2].imshow(binpred[0], cmap=\"Greens\", interpolation=\"none\")\n",
    "ax[2].set_title(\"Predicted mask\", fontsize=10)\n",
    "ax[2].get_yaxis().set_visible(False)\n",
    "\n",
    "# Plot RGBA image on the right\n",
    "ax[3].imshow(rgba_image, interpolation=\"none\")\n",
    "ax[3].set_title(\"Recovered Pixels and Gradient\", fontsize=10)\n",
    "ax[3].get_yaxis().set_visible(False)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"red\", edgecolor=\"red\", label=\"Recovered\"),\n",
    "    Patch(facecolor=\"blue\", edgecolor=\"blue\", label=\"Pred + Margin\"),\n",
    "]\n",
    "\n",
    "ax[3].legend(\n",
    "    handles=legend_elements, fontsize=8, loc=\"center left\", bbox_to_anchor=(1, 0.8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup conformalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkerie.models import UniversegInferenceWrap\n",
    "\n",
    "inferencer = UniversegInferenceWrap(\n",
    "    model=model,\n",
    "    support_images=support_images,\n",
    "    support_labels=support_labels,\n",
    "    device=device,\n",
    "    return_numpy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup conformal parameters\n",
    "- `covratio` $\\in (0,1)$ (coverage ratio): how many ground-truth pixels must be covered in each image to be considered a success\n",
    "- `alpha` $ \\in (0,1)$ (nominal risk): on average, how many inferences can be wrong (but we don't say by how much)\n",
    "\n",
    "During conformalization, the nonconformity scores tell\n",
    "- how many dilations, or \n",
    "- how big the structuring element (e.g. disk), or\n",
    "- which threshold $\\in (0,1)$\n",
    "\n",
    "was necessary to achieve the specified `covratio`. If `covratio = 1`, then 100% of the ground-truth pixels must be captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covratio = 0.99999\n",
    "# covratio = 0.999\n",
    "covratio = 0.99\n",
    "# covratio = 1.0\n",
    "\n",
    "# alpha = 0.90\n",
    "# alpha = 0.50\n",
    "alpha = 0.10\n",
    "# alpha = 0.80  # BAD just for testing\n",
    "\n",
    "score_funcs = [\"fixed_disk\", \"variable_disk\", \"thresholding\"]\n",
    "# chosen_nc_score = score_funcs[0]\n",
    "chosen_nc_score = score_funcs[1]\n",
    "# chosen_nc_score = score_funcs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute nonconformity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" --- n calibration points: {len(data_calib)}\")\n",
    "\n",
    "cpred = Conformalizer(\n",
    "    inferencer=inferencer,\n",
    "    nonconformity_function_name=chosen_nc_score,\n",
    "    structuring_element_params=se_params,\n",
    ")\n",
    "\n",
    "confo_resu = cpred.compute_nonconformity_scores(data_calib, covratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute conformalizing empirical quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" --- Required coverage ratio = {covratio}\")\n",
    "print(f\" --- Probability of succes on test = {1 - alpha}\")\n",
    "print(f\" --- Chosen nonconformity score: {chosen_nc_score}\")\n",
    "# cpred.plot_nc_scores(confo_resu[\"nonconformity\"], alpha)\n",
    "cpred.plot_nc_scores(alpha_risk=alpha)\n",
    "cpred.plot_nc_scores_frequency(alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics of conformal prediction sets on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" --- Required coverage ratio = {covratio}\")\n",
    "print(f\" --- Probability of succes on test = {1 - alpha}\")\n",
    "print()\n",
    "test_preds = cpred.test_inferences(data_test)\n",
    "test_results = cpred.test_conformalization(test_preds, alpha, covratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_coverage = np.mean(test_results.conformal_tests)\n",
    "empirical_covratio = np.mean(test_results.empirical_covratios)\n",
    "empirical_avg_add_pixels = np.mean(test_results.added_pixels)\n",
    "empirical_avg_stretch = np.mean(test_results.stretch_factors)\n",
    "\n",
    "print(\n",
    "    f\" --- cov : {empirical_coverage:.4f} vs nominal : {1-alpha}, for num elements: {len(test_results.conformal_tests)}\"\n",
    ")\n",
    "print(f\" --- {empirical_covratio = :.4f} -vs- {covratio = :.4f}\")\n",
    "print(f\" --- {empirical_avg_add_pixels = :.4f}\")\n",
    "print(f\" --- {empirical_avg_stretch = :.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
