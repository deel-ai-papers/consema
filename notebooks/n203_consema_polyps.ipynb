{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consema on Polyps data\n",
    "\n",
    "- Author: Luca Mossina. IRT Saint ExupÃ©ry, Toulouse, France\n",
    "\n",
    "**Scope**: test the full pipeline for Conformal Prediction on Polyps data and PraNet predictions (via repos of `aangelopouls`)\n",
    "\n",
    "- data: Polyps\n",
    "- predictor: PraNet as pre-computed by `aangelopouls` [(link to repo)](https://github.com/aangelopoulos/conformal-prediction/blob/67f506e4880e192ef9fc6a2de73e21b277f8c544/notebooks/tumor-segmentation.ipynb)\n",
    "- nonconformity score: thresholding, n. of dilations, radius of structuring element\n",
    "- generation of dataset: https://github.com/aangelopoulos/conformal-prediction/blob/67f506e4880e192ef9fc6a2de73e21b277f8c544/generation-scripts/generate-polyps.py#L56\n",
    "\n",
    "**REMARK**: if we want to be able to plot prediction, ground-truth masks and the input images, we must restrict our attention to the 500 samples in `data[example_indexes]`.(see below).\n",
    "They were sampled randomly without replacement (see link above `generation of dataset`).\n",
    "\n",
    "Hence, we should be able to safely split the example in two parts, one for calibration and the other for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from consema.conformal import Conformalizer\n",
    "import numpy as np\n",
    "import os\n",
    "from consema.morphology import (\n",
    "    operator_dilation_sequential,\n",
    "    operator_dilation_disk_radius,\n",
    ")\n",
    "\n",
    "device_str = \"cpu\"\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data, predictors and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_polyps(random_seed: int, cal_test_ratio: float):\n",
    "    # load data as downloaded from gdrive\n",
    "    load_dotenv()\n",
    "\n",
    "    POLYPS_NPZ = os.getenv(\"POLYPS_NPZ\")\n",
    "    print(f\"{POLYPS_NPZ = }\")\n",
    "\n",
    "    data = np.load(f\"{POLYPS_NPZ}/polyps-pranet.npz\")\n",
    "    sgmd = data[\"sgmd\"]  # sigmoid scores\n",
    "\n",
    "    gt_masks = data[\"targets\"]\n",
    "    test_calib_idxs = [elem.item() for elem in data[\"example_indexes\"]]\n",
    "\n",
    "    # Rescale sigmoid scores to [0, 1] for each image\n",
    "    sgmd_rescaled = np.array(\n",
    "        [(img - img.min()) / (img.max() - img.min()) for img in sgmd]\n",
    "    )\n",
    "    sgmd = sgmd_rescaled\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(test_calib_idxs)\n",
    "\n",
    "    split_num = int(len(test_calib_idxs) * cal_test_ratio)\n",
    "    calib_idxs = test_calib_idxs[:split_num]\n",
    "    test_idxs = test_calib_idxs[split_num:]\n",
    "\n",
    "    assert set(calib_idxs).isdisjoint(\n",
    "        set(test_idxs)\n",
    "    ), \"calib and test sets are not disjoint\"\n",
    "\n",
    "    n_samples = len(calib_idxs)\n",
    "    print(f\"{n_samples = }\")\n",
    "\n",
    "    # return sgmd, gt_masks, calib_idxs, test_idxs\n",
    "    from skimage.transform import resize\n",
    "\n",
    "    calib_pred_arrays = sgmd[calib_idxs]\n",
    "    calib_gt_arrays = gt_masks[calib_idxs]\n",
    "    calib_input_img_paths = [\n",
    "        f\"{POLYPS_NPZ}/examples/\" + str(rnd_idx_) + \".jpg\"\n",
    "        for rnd_idx_ in test_calib_idxs\n",
    "    ]\n",
    "    calib_images = [plt.imread(img_path_) for img_path_ in calib_input_img_paths]\n",
    "\n",
    "    calib_gt_img_paths = [\n",
    "        f\"{POLYPS_NPZ}/examples/\" + str(rnd_idx_) + \"_gt_mask.jpg\"\n",
    "        for rnd_idx_ in test_calib_idxs\n",
    "    ]\n",
    "    calib_gt_imgs = [plt.imread(img_path_) for img_path_ in calib_gt_img_paths]\n",
    "\n",
    "    calib_preds_resized = [\n",
    "        resize(pred, (img.shape[0], img.shape[1]), anti_aliasing=False)\n",
    "        for pred, img in zip(calib_pred_arrays, calib_images)\n",
    "    ]\n",
    "    calib_gt_resized = [\n",
    "        resize(gt, (img.shape[0], img.shape[1]), anti_aliasing=False)\n",
    "        for gt, img in zip(calib_gt_arrays, calib_images)\n",
    "    ]\n",
    "\n",
    "    # DO_TEST = False\n",
    "    DO_TEST = True\n",
    "\n",
    "    if DO_TEST:\n",
    "        test_pred_arrays = sgmd[test_idxs]\n",
    "        test_gt_arrays = gt_masks[test_idxs]\n",
    "        test_input_img_paths = [\n",
    "            f\"{POLYPS_NPZ}/examples/\" + str(rnd_idx_) + \".jpg\" for rnd_idx_ in test_idxs\n",
    "        ]\n",
    "        test_images = [plt.imread(img_path_) for img_path_ in test_input_img_paths]\n",
    "\n",
    "        test_gt_img_paths = [\n",
    "            f\"{POLYPS_NPZ}/examples/\" + str(rnd_idx_) + \"_gt_mask.jpg\"\n",
    "            for rnd_idx_ in test_idxs\n",
    "        ]\n",
    "        test_gt_imgs = [plt.imread(img_path_) for img_path_ in test_gt_img_paths]\n",
    "\n",
    "        test_preds_resized = [\n",
    "            resize(pred, (img.shape[0], img.shape[1]), anti_aliasing=False)\n",
    "            for pred, img in zip(test_pred_arrays, test_images)\n",
    "        ]\n",
    "        test_gt_resized = [\n",
    "            resize(gt, (img.shape[0], img.shape[1]), anti_aliasing=False)\n",
    "            for gt, img in zip(test_gt_arrays, test_images)\n",
    "        ]\n",
    "    else:\n",
    "        test_images = []\n",
    "        test_gt_arrays = []\n",
    "        test_pred_arrays = []\n",
    "        test_preds_resized = []\n",
    "        test_gt_resized = []\n",
    "\n",
    "    # add a dummy dimenstion for batch size (== 1)\n",
    "    calib_pred_arrays = [np.expand_dims(pred, 0) for pred in calib_pred_arrays]\n",
    "    calib_gt_arrays = [np.expand_dims(gt, 0) for gt in calib_gt_arrays]\n",
    "    calib_preds_resized = [np.expand_dims(pred, 0) for pred in calib_preds_resized]\n",
    "    calib_gt_resized = [np.expand_dims(gt, 0) for gt in calib_gt_resized]\n",
    "    calib_images = [np.expand_dims(img, 0) for img in calib_images]\n",
    "\n",
    "    if DO_TEST:\n",
    "        test_pred_arrays = [np.expand_dims(pred, 0) for pred in test_pred_arrays]\n",
    "        test_gt_arrays = [np.expand_dims(gt, 0) for gt in test_gt_arrays]\n",
    "        test_preds_resized = [np.expand_dims(pred, 0) for pred in test_preds_resized]\n",
    "        test_gt_resized = [np.expand_dims(gt, 0) for gt in test_gt_resized]\n",
    "        test_images = [np.expand_dims(img, 0) for img in test_images]\n",
    "    else:\n",
    "        test_images = []\n",
    "        test_gt_arrays = []\n",
    "        test_pred_arrays = []\n",
    "        test_preds_resized = []\n",
    "        test_gt_resized = []\n",
    "\n",
    "    return (\n",
    "        calib_images,\n",
    "        calib_gt_arrays,\n",
    "        calib_pred_arrays,\n",
    "        test_images,\n",
    "        test_gt_arrays,\n",
    "        test_pred_arrays,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    calib_images,\n",
    "    calib_gt_arrays,\n",
    "    calib_pred_arrays,\n",
    "    test_images,\n",
    "    test_gt_arrays,\n",
    "    test_pred_arrays,\n",
    ") = setup_polyps(RANDOM_SEED, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize False Negatives: we want to control their quantity\n",
    "\n",
    "Via conformal prediction, we control how many false negative we will have, on average, in our test inferences (e.g. when deployed in production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.plots import visualize_false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, (image, prediction, gt) in enumerate(\n",
    "    zip(calib_images, calib_pred_arrays, calib_gt_arrays)\n",
    "):\n",
    "    if it >= 2:\n",
    "        break\n",
    "    visualize_false_negatives(image[0], gt[0], prediction[0] > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, the **red points** are false negatives, that is, points that belong to the ground-truth masks but were not predicted by the algorithm.\n",
    "\n",
    "Using conformal prediction, we want to \"extend\" the prediction area (in green) with a conformal margin so that we limit our prediction errors, that is, we reduce the number of false negative with high probability at a significance level chosen by the user.\n",
    "\n",
    "The price to pay are more false positives: in this case, we can imagine that false positives push the users to be more conservative and they are \"safe\" errors.\n",
    "\n",
    "If there are too many false positives, then the prediction becomes operationally useless; this tradeoff is controlled by the user and their predictive model: worse models will have more false negatives, hence larger conformal margins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.morphology import (\n",
    "    dilation_metrics,\n",
    ")\n",
    "from consema.conformal import thresholding_score\n",
    "from consema.plots import plot_margin_and_recovered, visualize_false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ = 100\n",
    "bintruth = calib_gt_arrays[idx_] > 0.5\n",
    "binpred = calib_pred_arrays[idx_] > 0.5\n",
    "softpred = calib_pred_arrays[idx_]\n",
    "\n",
    "cov_threshold = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(softpred[0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding nonconformity score\n",
    "\n",
    "Sources:\n",
    "- LAC (Least Ambiguous Set-Valued Classifiers), [Sadinle et al. (2019)](https://arxiv.org/abs/1609.00451)\n",
    "- CRC (Conformal Risk Control), [Angelopoulos et al. (2022)](https://arxiv.org/abs/2208.02814)\n",
    "\n",
    "**Thresholding the softmax** or sigmoid scores for **classification** follows from the theory developed in Sadinle et al. (2019), where they work with generic probability distribution, not specifically with neural networks. In this case, the probability estimates (e.g. softmax scores) are known to be not calibrated, and some properties (e.g. optimal size of prediction sets) do not necessarily hold. \n",
    "\n",
    "As for **binary image segmentation** with an underlying pixel-wise classifier, this was introduced as an application of their CRC algorithm by Angelopoulos et al. (2022): they do not explicitly mention nonconformity scores, although it follows immediately from their definition of risk and computation of $\\hat{\\lambda}$, for the case of binary (conformal) losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_threshold = thresholding_score(\n",
    "    gt_mask_=bintruth,\n",
    "    soft_pred_mask_=softpred,\n",
    "    # se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "\n",
    "from consema.morphology import operator_thresholding\n",
    "\n",
    "dilated_threshold_mask = operator_thresholding(\n",
    "    softpred[0], nc_threshold, se_params_=None\n",
    ")\n",
    "\n",
    "print(f\"{nc_threshold = :.9f}\")\n",
    "metrics = dilation_metrics(dilated_threshold_mask[0], binpred[0])\n",
    "print(f\" --- THRESH: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "\n",
    "coverage_tensor = np.multiply(dilated_threshold_mask, bintruth)\n",
    "coverage = np.sum(coverage_tensor) / np.count_nonzero(bintruth)\n",
    "print(f\"{coverage = :.9f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dilated_threshold_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_threshold_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_margin_and_recovered(\n",
    "    binpred[0],\n",
    "    bintruth[0],\n",
    "    dilated_threshold_mask[0],\n",
    "    plot_hard_margin=False,\n",
    "    figsize=(11, 5),\n",
    "    input_image=calib_images[idx_][0],\n",
    "    softprediction=softpred[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological prediction sets: dilation\n",
    "\n",
    "This is part of the original contribution of the paper: building statistically valid prediction sets under minimal information.\n",
    "- No softmax/sigmoid scores necessary\n",
    "- usable on blackbox predictor, e.g. hidden behind API, MLaaS, etc.\n",
    "- only need a small dataset of labeled, production-like data to measure uncertainty\n",
    "- literature available: mathematical morphology for computer vision (without deep learning)\n",
    "\n",
    "I introduce two cases:\n",
    "- (3x3) structuring element, either square or cross: iteratively dilate the mask as dilated at previous iterations until $\\tau \\times 100 \\%$ of the ground-truth pixels are recovered. Applying several dilations (or other) is also known as **depth** of the dilation. This will constitue the nonconformity score, the higher the worse the prediction was.\n",
    "- variable-size disk: as above, but only apply one dilation to the predicted mask, with the radius of the disk increasing at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.morphology import (\n",
    "    dilation_score_variable_disk,\n",
    "    dilation_score_fixed_disk,\n",
    ")\n",
    "\n",
    "se_params = dict(strict_radius=True)  # [3 X 3] cross\n",
    "# se_params = dict(strict_radius=False)  # [3 X 3] square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative dilations\n",
    "\n",
    "Nonconcormity score = depth, or number of repeated dilations.\n",
    "- Using fixed-size structuring element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Fixed disk\n",
    "# nc_fixed, dilated_mask_fixed = dilation_score_fixed_disk(\n",
    "nc_fixed = dilation_score_fixed_disk(\n",
    "    gt_mask_=bintruth[0],\n",
    "    pred_mask_=binpred[0],\n",
    "    se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "print(f\"{nc_fixed = }\")\n",
    "\n",
    "\n",
    "dilated_mask_fixed = operator_dilation_sequential(\n",
    "    input_mask=binpred[0], operator_parameter=nc_fixed, se_params_=se_params\n",
    ")\n",
    "metrics = dilation_metrics(dilated_mask_fixed, binpred[0])\n",
    "print(f\" ---  FIXED: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "plot_margin_and_recovered(\n",
    "    binpred[0],\n",
    "    bintruth[0],\n",
    "    dilated_mask_fixed,\n",
    "    input_image=calib_images[idx_][0],\n",
    "    softprediction=softpred[0],\n",
    ")\n",
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_mask_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable-size disk as structuring element\n",
    "\n",
    "Instead of repeating dilations incrementally, on can obtain a similar results modifying the radius of the structuring element used for dilations.\n",
    "\n",
    "Computationally, it should be worse than fixed-size for big radii: a naive implementation requires a number a computations that grows quadratically with the size (H x W) of the structuring element.\n",
    "\n",
    "For binary dilations (our case), one can use a convolution followed by a max: this can be a good option if working directly with torch and tensors; with large images, we could expect comp. gains via gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Variable disk\n",
    "nc_variable = dilation_score_variable_disk(\n",
    "    gt_mask_=bintruth[0],\n",
    "    pred_mask_=binpred[0],\n",
    "    se_params_=se_params,\n",
    "    coverage_threshold=cov_threshold,\n",
    ")\n",
    "\n",
    "print(f\"{nc_variable = }\")\n",
    "\n",
    "dilated_mask_variable = operator_dilation_disk_radius(\n",
    "    input_mask=binpred[0], operator_parameter=nc_variable, se_params_=se_params\n",
    ")\n",
    "\n",
    "visualize_false_negatives(calib_images[idx_][0], bintruth[0], dilated_mask_variable)\n",
    "\n",
    "metrics = dilation_metrics(dilated_mask_variable, binpred[0])\n",
    "print(f\" ---  VARIA: n. added px = {metrics[0]}, stretch = {metrics[1]}\")\n",
    "plot_margin_and_recovered(binpred[0], bintruth[0], dilated_mask_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal loop: compute scores and quantiles\n",
    "\n",
    "Conformal prediction (CP) is usually presented as a predictive method to build statistically valid prediction sets (or intervals) that will contain, with high probability, the true value of a target $Y$ being predicted by some ML model as $\\hat{Y} = \\hat{f}(X)$; the features X could be tabular data, images, etc.\n",
    "\n",
    "However, we have another useful piece of information: the actual errors (nonconformity scores), their size, their distribution.\n",
    "CP can also be used as a diagnostic tool: the users choose a score (or make their own), a tolerable risk level, and then they could:\n",
    "- compare several architectures, trained on the same data: the one with the smallest scores (and prediction set) could be preferable to another with slightly better performance metrics (precision, mAP, etc.)\n",
    "- interpret the errors: in production, how and where the model errs.\n",
    "- interpret the size and shape of the prediction set: for regression or classification, this is the width and cardinality or pred. sets. For segmentation or object detection, this could be the size of the conformal margins, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize margin: use a gradient to show successive dilations and recovered pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.plots import margin_gradient_visu\n",
    "\n",
    "margin_var = margin_gradient_visu(\n",
    "    binpred[0], \"variable_disk\", 10, None, False, se_params\n",
    ")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "ax[0].imshow(margin_var, cmap=\"cividis\", interpolation=\"none\")\n",
    "margin_fixed = margin_gradient_visu(binpred[0], \"fixed_disk\", 10, None, True, se_params)\n",
    "ax[1].imshow(margin_fixed, cmap=\"cividis\", interpolation=\"none\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from consema.conformal import recovered_pixels_bin_array\n",
    "\n",
    "_grad = margin_gradient_visu(binpred[0], \"variable_disk\", 10, 20, True, se_params)\n",
    "\n",
    "# Assuming _grad is your H x W array\n",
    "H, W = _grad.shape\n",
    "rgba_image = np.zeros((H, W, 4), dtype=np.uint8)  # Create an RGBA array\n",
    "\n",
    "# Normalize the gradient to the range [0, 1]\n",
    "normalized_grad = _grad / np.max(_grad)\n",
    "\n",
    "# Set all pixels that are non-zero in the recovered mask to red with full opacity\n",
    "recovered = recovered_pixels_bin_array(bintruth[0], binpred[0], dilated_mask_variable)\n",
    "\n",
    "# Define blending factor and colors\n",
    "alpha_blend = 0.7  # Controls red intensity (0 = no red, 1 = fully red)\n",
    "\n",
    "# Assign colors: white for zeros, blue for non-zero entries\n",
    "nonmargin_indices = np.where(_grad > 0)\n",
    "rgba_image[..., 0] = (1 - normalized_grad) * 255  # Red channel\n",
    "rgba_image[..., 1] = (1 - normalized_grad) * 255  # Green channel\n",
    "rgba_image[..., 2] = 255  # Blue channel\n",
    "rgba_image[..., 3] = (normalized_grad * 255).astype(\n",
    "    np.uint8\n",
    ")  # Alpha channel (transparency)\n",
    "\n",
    "# Blend red for recovered pixels\n",
    "recovered_indices = np.where(recovered == 1)\n",
    "rgba_image[recovered_indices] = (\n",
    "    alpha_blend * np.array([255, 0, 0, 255])  # Red with full opacity\n",
    "    + (1 - alpha_blend) * rgba_image[recovered_indices]\n",
    ").astype(np.uint8)\n",
    "\n",
    "# Visualize the RGBA image\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "# Plot ground truth mask on the left\n",
    "ax[0].imshow(bintruth[0], cmap=\"gray\", interpolation=\"none\")\n",
    "ax[0].set_title(\"Ground-truth mask\", fontsize=10)\n",
    "# ax[0].axis(\"off\")\n",
    "\n",
    "# Plot softmax prediction\n",
    "ax[1].imshow(softpred[0], cmap=\"viridis\", interpolation=\"none\")\n",
    "ax[1].set_title(\"Softmax Prediction\", fontsize=10)\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "# plot hard predicted mask\n",
    "ax[2].imshow(binpred[0], cmap=\"gray\", interpolation=\"none\")\n",
    "# ax[2].imshow(binpred[0], cmap=\"Greens\", interpolation=\"none\")\n",
    "ax[2].set_title(\"Predicted mask\", fontsize=10)\n",
    "ax[2].get_yaxis().set_visible(False)\n",
    "\n",
    "# Plot RGBA image on the right\n",
    "ax[3].imshow(rgba_image, interpolation=\"none\")\n",
    "ax[3].set_title(\"Recovered Pixels and Gradient\", fontsize=10)\n",
    "ax[3].get_yaxis().set_visible(False)\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"red\", edgecolor=\"red\", label=\"Recovered\"),\n",
    "    Patch(facecolor=\"blue\", edgecolor=\"blue\", label=\"Pred + Margin\"),\n",
    "]\n",
    "\n",
    "ax[3].legend(\n",
    "    handles=legend_elements, fontsize=8, loc=\"center left\", bbox_to_anchor=(1, 0.8)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup conformalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkerie.models import PranetPolypsPrecomputedInferencer\n",
    "\n",
    "inferencer = PranetPolypsPrecomputedInferencer(device=device_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup conformal parameters\n",
    "- `covratio` $\\in (0,1)$ (coverage ratio): how many ground-truth pixels must be covered in each image to be considered a success\n",
    "- `alpha` $ \\in (0,1)$ (nominal risk): on average, how many inferences can be wrong (but we don't say by how much)\n",
    "\n",
    "During conformalization, the nonconformity scores tell\n",
    "- how many dilations, or \n",
    "- how big the structuring element (e.g. disk), or\n",
    "- which threshold $\\in (0,1)$\n",
    "\n",
    "was necessary to achieve the specified `covratio`. If `covratio = 1`, then 100% of the ground-truth pixels must be captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calib = tuple(\n",
    "    (img, gt, softpred)\n",
    "    for img, gt, softpred in zip(calib_images, calib_gt_arrays, calib_pred_arrays)\n",
    ")\n",
    "\n",
    "data_test = tuple(\n",
    "    (img, gt, softpred)\n",
    "    for img, gt, softpred in zip(test_images, test_gt_arrays, test_pred_arrays)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_funcs = [\"fixed_disk\", \"variable_disk\", \"thresholding\"]\n",
    "\n",
    "se_params = dict(strict_radius=True)  # [3 X 3] cross\n",
    "# se_params = dict(strict_radius=False)  # [3 X 3] square\n",
    "\n",
    "configs = [\n",
    "    dict(\n",
    "        chosen_nc_score=score_funcs[0],\n",
    "        covratio=0.99,\n",
    "        alpha=0.1,\n",
    "        se_params=se_params,\n",
    "    ),\n",
    "    dict(\n",
    "        chosen_nc_score=score_funcs[2],\n",
    "        covratio=0.99,\n",
    "        alpha=0.1,\n",
    "        se_params=se_params,\n",
    "    ),\n",
    "]\n",
    "\n",
    "conformers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" --- n calibration points: {len(data_calib)}\")\n",
    "\n",
    "cp_quantiles = []\n",
    "\n",
    "for cfg in configs:\n",
    "    cpred = Conformalizer(\n",
    "        inferencer=inferencer,\n",
    "        nonconformity_function_name=cfg[\"chosen_nc_score\"],\n",
    "        structuring_element_params=cfg[\"se_params\"],\n",
    "    )\n",
    "\n",
    "    confo_resu = cpred.compute_nonconformity_scores(data_calib, cfg[\"covratio\"])\n",
    "\n",
    "    conformalizing_quantile = cpred.get_conformal_quantile(cfg[\"alpha\"])\n",
    "    print(f\"{conformalizing_quantile = }\")\n",
    "    cp_quantiles.append(conformalizing_quantile)\n",
    "\n",
    "    # is_thresholding = chosen_nc_score == \"thresholding\"\n",
    "    # # cpred.plot_nc_scores(alpha_risk=alpha, threshold_score=is_thresholding)\n",
    "    # # cpred.plot_nc_scores_frequency(alpha=alpha)\n",
    "\n",
    "    test_predictions = cpred.test_inferences(data_test)\n",
    "    test_results = cpred.test_conformalization(\n",
    "        test_predictions, cfg[\"alpha\"], cfg[\"covratio\"]\n",
    "    )\n",
    "\n",
    "    empirical_coverage = np.mean(test_results.conformal_tests)\n",
    "    empirical_covratio = np.mean(test_results.empirical_covratios)\n",
    "    empirical_avg_add_pixels = np.mean(test_results.added_pixels)\n",
    "    empirical_avg_stretch = np.mean(test_results.stretch_factors)\n",
    "\n",
    "    print(\n",
    "        f\" --- cov : {empirical_coverage:.4f} vs nominal : {1-cfg['alpha']}, for num elements: {len(test_results.conformal_tests)}\"\n",
    "    )\n",
    "    print(f\" --- {empirical_avg_add_pixels = :.4f}\")\n",
    "    print(f\" --- {empirical_avg_stretch = :.4f}\")\n",
    "\n",
    "    conformers.append((cpred, test_predictions, test_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## func to plot Fig.2 in paper\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def fig_plot_margin_and_recovered(\n",
    "    predicted_mask: np.ndarray,\n",
    "    ground_truth_mask: np.ndarray,\n",
    "    dilated_mask: np.ndarray,\n",
    "    input_image: Optional[np.ndarray] = None,\n",
    "    softprediction: Optional[np.ndarray] = None,\n",
    "    plot_hard_margin: Optional[bool] = False,\n",
    "    figsize: Optional[tuple] = None,\n",
    "    save_cp_method_name: Optional[str] = None,\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "    if isinstance(predicted_mask, torch.Tensor):\n",
    "        predicted_mask = predicted_mask.cpu().numpy()\n",
    "    if isinstance(ground_truth_mask, torch.Tensor):\n",
    "        ground_truth_mask = ground_truth_mask.cpu().numpy()\n",
    "    if isinstance(dilated_mask, torch.Tensor):\n",
    "        dilated_mask = dilated_mask.cpu().numpy()\n",
    "\n",
    "    nplots = 4\n",
    "    if input_image is not None:\n",
    "        nplots += 1\n",
    "    if softprediction is not None:\n",
    "        nplots += 1\n",
    "\n",
    "    margin = np.logical_xor(dilated_mask, predicted_mask)\n",
    "\n",
    "    # ## for readability in paper, crop the image.\n",
    "    # ## DANGER: assumes there is nothing going on in this region\n",
    "    # crop = 1  # 15\n",
    "    # ground_truth_mask = ground_truth_mask[crop:-crop, crop:-crop]\n",
    "    # predicted_mask = predicted_mask[crop:-crop, crop:-crop]\n",
    "    # dilated_mask = dilated_mask[crop:-crop, crop:-crop]\n",
    "    # margin = margin[crop:-crop, crop:-crop]\n",
    "\n",
    "    # ## do not crop input image, soft pred\n",
    "    # # if input_image is not None:\n",
    "    # #     input_image = input_image[crop:-crop, crop:-crop]\n",
    "\n",
    "    recovered = np.where(\n",
    "        (ground_truth_mask == 1) & (predicted_mask == 0) & (dilated_mask == 1), 1, 0\n",
    "    )\n",
    "    not_recovered = np.where(\n",
    "        (ground_truth_mask == 1) & (predicted_mask == 0) & (dilated_mask == 0), 666, 0\n",
    "    )\n",
    "\n",
    "    if recovered.sum() == 0:\n",
    "        print(\"No pixels were recovered\")\n",
    "        # return\n",
    "\n",
    "    recovered_rgba = np.zeros((*recovered.shape, 4))\n",
    "    # (X,X,X,1) for 1 with full opacity\n",
    "    recovered_rgba[recovered == 1] = [1, 0, 0, 1]\n",
    "    recovered_rgba[recovered == 0] = [0, 0, 0, 0]  # Transparent for value 0\n",
    "    recovered_rgba[not_recovered == 666] = [0, 100 / 255, 0, 1]\n",
    "\n",
    "    if plot_hard_margin:\n",
    "        margin = np.where(margin > 0, 1, 0)\n",
    "\n",
    "    figsize = (5 * nplots, 6)\n",
    "    fig, axes = plt.subplots(1, nplots, figsize=figsize, dpi=200)\n",
    "\n",
    "    ## === (1) Ground-truth mask\n",
    "    gt_color = np.array((216, 27, 96)) / 255\n",
    "    gt_rgba = np.zeros((*ground_truth_mask.shape, 4))\n",
    "    gt_rgba[ground_truth_mask == 1] = [gt_color[0], gt_color[1], gt_color[2], 1]\n",
    "    gt_rgba[ground_truth_mask == 0] = [1, 1, 1, 1]  # White for value 0\n",
    "\n",
    "    axes[0].imshow(gt_rgba, interpolation=\"none\")\n",
    "    # axes[0].set_title(\"Ground-truth mask\")\n",
    "    axes[0].set_xticks([])\n",
    "    axes[0].set_yticks([])\n",
    "\n",
    "    ## === (2) Predicted mask\n",
    "    pred_color = np.array((30, 136, 229)) / 255\n",
    "    pred_rgba = np.zeros((*predicted_mask.shape, 4))\n",
    "    pred_rgba[predicted_mask == 1] = [pred_color[0], pred_color[1], pred_color[2], 0.75]\n",
    "    pred_rgba[predicted_mask == 0] = [1, 1, 1, 1]  # White for value 0\n",
    "\n",
    "    axes[1].imshow(pred_rgba, interpolation=\"none\")\n",
    "    # axes[1].set_title(\"Predicted mask\")\n",
    "    axes[1].set_xticks([])\n",
    "    axes[1].set_yticks([])\n",
    "\n",
    "    ## === (3) Intersection of gt and pred: intersection in true_pos_color, the others as gt or pred respectively\n",
    "    true_pos_color = np.array((120, 94, 240)) / 255\n",
    "    intersection_rgba = np.zeros((*ground_truth_mask.shape, 4))\n",
    "    intersection_rgba[(ground_truth_mask == 1) & (predicted_mask == 1)] = [\n",
    "        true_pos_color[0],\n",
    "        true_pos_color[1],\n",
    "        true_pos_color[2],\n",
    "        0.8,\n",
    "    ]\n",
    "    # false negative in gt_color\n",
    "    intersection_rgba[(ground_truth_mask == 1) & (predicted_mask == 0)] = [\n",
    "        gt_color[0],\n",
    "        gt_color[1],\n",
    "        gt_color[2],\n",
    "        1,\n",
    "    ]\n",
    "    # false positive in pred_color\n",
    "    intersection_rgba[(ground_truth_mask == 0) & (predicted_mask == 1)] = [\n",
    "        pred_color[0],\n",
    "        pred_color[1],\n",
    "        pred_color[2],\n",
    "        0.75,\n",
    "    ]\n",
    "\n",
    "    axes[2].imshow(intersection_rgba, interpolation=\"none\")\n",
    "    # axes[2].set_title(\"Intersection\")\n",
    "    axes[2].set_xticks([])\n",
    "    axes[2].set_yticks([])\n",
    "\n",
    "    ## === (4) Dilated mask:\n",
    "    ## pred_mask in \"gainsboro\", margin in pred_color with alpha=0.5, recovered in orange with hatching\n",
    "    recov_color = np.array((254, 97, 0)) / 255\n",
    "    dilated_rgba = np.zeros((*dilated_mask.shape, 4))\n",
    "    dilated_rgba[margin == 1] = [pred_color[0], pred_color[1], pred_color[2], 0.5]\n",
    "    dilated_rgba[recovered == 1] = [recov_color[0], recov_color[1], recov_color[2], 1]\n",
    "    # dilated_rgba[predicted_mask == 1] = [0.86, 0.86, 0.86, 1]\n",
    "    dilated_rgba[predicted_mask == 1] = [\n",
    "        true_pos_color[0],\n",
    "        true_pos_color[1],\n",
    "        true_pos_color[2],\n",
    "        0.8,\n",
    "    ]\n",
    "\n",
    "    axes[3].imshow(dilated_rgba, interpolation=\"none\")\n",
    "    # axes[3].set_title(\"Dilated mask\")\n",
    "    axes[3].set_xticks([])\n",
    "    axes[3].set_yticks([])\n",
    "\n",
    "    ## if input_image is not None, plot it\n",
    "    if input_image is not None:\n",
    "        axes[4].imshow(input_image, cmap=\"Greys_r\", interpolation=\"none\")\n",
    "        # axes[4].set_title(\"Input image\")\n",
    "        axes[4].set_xticks([])\n",
    "        axes[4].set_yticks([])\n",
    "        # axes[4].set_xlabel(\"Input image\")\n",
    "        ## if softprediction is not None, plot it\n",
    "        if softprediction is not None:\n",
    "            im = axes[5].imshow(softprediction, cmap=\"viridis\", interpolation=\"none\")\n",
    "            axes[5].set_xticks([])\n",
    "            axes[5].set_yticks([])\n",
    "            divider = make_axes_locatable(axes[5])\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            cbar = fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            cbar.ax.tick_params(\n",
    "                labelsize=16\n",
    "            )  # Increase the font size of the colorbar labels\n",
    "            cbar.set_ticks([0, 0.5, 1])  # Set the ticks to 0, 0.5, and 1\n",
    "            # cbar.set_label('Color Intensity')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save individual subplots\n",
    "    fig_filenames = [\n",
    "        \"figures/dil_vs_thresh/polyps_comparison_01_gt.png\",\n",
    "        \"figures/dil_vs_thresh/polyps_comparison_02_pred.png\",\n",
    "        \"figures/dil_vs_thresh/polyps_comparison_03_intersection.png\",\n",
    "        f\"figures/dil_vs_thresh/polyps_comparison_04_dilated_{save_cp_method_name}.png\",\n",
    "        \"figures/dil_vs_thresh/polyps_comparison_05_input.png\",\n",
    "        \"figures/dil_vs_thresh/polyps_comparison_06_soft_prediction.png\",\n",
    "    ]\n",
    "    for i, ax in enumerate(axes[:-1]):\n",
    "        extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "        fig.savefig(\n",
    "            fig_filenames[i],\n",
    "            bbox_inches=extent.expanded(1.025, 1.025),\n",
    "            pad_inches=0.08,\n",
    "            dpi=200,\n",
    "        )\n",
    "\n",
    "    # Save the last figure with the colorbar\n",
    "    extent = axes[5].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(\n",
    "        fig_filenames[5],\n",
    "        bbox_inches=extent.expanded(\n",
    "            1.5, 1.10\n",
    "        ),  # Adjust the expansion to include the colorbar\n",
    "        pad_inches=0.08,\n",
    "        dpi=200,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from consema.plots import plot_margin_and_recovered\n",
    "\n",
    "for idx in range(1):\n",
    "    test_image = test_images[idx][0]\n",
    "    input_image = test_image\n",
    "    test_gt = test_gt_arrays[idx][0]\n",
    "    test_pred = test_pred_arrays[idx][0]\n",
    "    hard_pred = test_pred > 0.5\n",
    "\n",
    "    # Compute the conformal set using the threshold as the conformal quantile\n",
    "    conformal_n_dilations = cp_quantiles[0]\n",
    "    conformal_threshold = cp_quantiles[1]\n",
    "\n",
    "    conformal_sets = [\n",
    "        operator_dilation_sequential(\n",
    "            input_mask=hard_pred,\n",
    "            operator_parameter=conformal_n_dilations,\n",
    "            se_params_=se_params,\n",
    "        ),\n",
    "        # operator_thresholding(pred_softmax=test_pred, operator_parameter=conformal_threshold, se_params_=se_params),\n",
    "        test_pred > 1 - conformal_threshold,\n",
    "    ]\n",
    "\n",
    "    # (1) Fixed disk\n",
    "    fig_plot_margin_and_recovered(\n",
    "        predicted_mask=hard_pred,\n",
    "        ground_truth_mask=test_gt,\n",
    "        dilated_mask=conformal_sets[0],\n",
    "        input_image=test_image,\n",
    "        softprediction=test_pred,\n",
    "        save_cp_method_name=\"dilation\",\n",
    "    )\n",
    "\n",
    "    # (2) threshold disk\n",
    "    fig_plot_margin_and_recovered(\n",
    "        predicted_mask=hard_pred,\n",
    "        ground_truth_mask=test_gt,\n",
    "        dilated_mask=conformal_sets[1],\n",
    "        input_image=test_image,\n",
    "        softprediction=test_pred,\n",
    "        save_cp_method_name=\"threshold\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
